{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from utilities1 import *\n",
    "import argparse\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import gzip\n",
    "from models.rnn import BasicRNN\n",
    "from models.td_rnn import TargetRNN\n",
    "import _pickle as pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_numpy(x):\n",
    "    ''' Need to cast before calling numpy()\n",
    "    '''\n",
    "    return x.data.type(torch.DoubleTensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip):\n",
    "    \"\"\"Computes a gradient clipping coefficient based on gradient norm.\"\"\"\n",
    "    totalnorm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:      #changed\n",
    "            modulenorm = p.grad.data.norm()\n",
    "        totalnorm += modulenorm ** 2\n",
    "    totalnorm = math.sqrt(totalnorm)\n",
    "    return min(1, clip / (totalnorm + 1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    # What's this?\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseExperiment:\n",
    "    ''' Implements a base experiment class for Aspect-Based Sentiment Analysis on SemEval 2014\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.uuid = datetime.now().strftime(\"%d_%H:%M:%S\")\n",
    "        self.parser = argparse.ArgumentParser()\n",
    "        self.parser.add_argument(\"--mode\", dest=\"mode\", type=str, metavar='<str>', default='term', help=\"Experiment Mode (term|aspect) (default=term)\")\n",
    "        self.parser.add_argument(\"--dataset\", dest=\"dataset\", type=str, metavar='<str>', default='Restaurants', help=\"Dataset (Laptop/Restaurants) (default=Restaurants)\")\n",
    "        self.parser.add_argument(\"--mdl\", dest=\"model_type\", type=str, metavar='<str>', default='RNN', help=\"(RNN|TD-RNN|ATT-RNN)\")\n",
    "        self.parser.add_argument(\"--rnn_type\", dest=\"rnn_type\", type=str, metavar='<str>', default='LSTM', help=\"Recurrent unit type (lstm|gru|simple) (default=lstm)\")\n",
    "        self.parser.add_argument(\"--term_mdl\", dest=\"term_model\", type=str, metavar='<str>', default='mean', help=\"Model type for term sequences (default=mean)\")\n",
    "        self.parser.add_argument(\"--opt\", dest=\"opt\", type=str, metavar='<str>', default='Adam', help=\"Optimization algorithm (rmsprop|sgd|adagrad|adadelta|adam|adamax) (default=rmsprop)\")\n",
    "        self.parser.add_argument(\"--emb_size\", dest=\"embedding_size\", type=int, metavar='<int>', default=300, help=\"Embeddings dimension (default=50)\")\n",
    "        self.parser.add_argument(\"--rnn_size\", dest=\"rnn_size\", type=int, metavar='<int>', default=300, help=\"RNN dimension. '0' means no RNN layer (default=300)\")\n",
    "        self.parser.add_argument(\"--batch-size\", dest=\"batch_size\", type=int, metavar='<int>', default=20, help=\"Batch size (default=256)\")\n",
    "        self.parser.add_argument(\"--rnn_layers\", dest=\"rnn_layers\", type=int, metavar='<int>', default=1, help=\"Number of RNN layers\")\n",
    "        self.parser.add_argument(\"--rnn_direction\", dest=\"rnn_direction\", type=str, metavar='<str>', default='uni', help=\"Direction of RNN\")\n",
    "        self.parser.add_argument(\"--aggregation\", dest=\"aggregation\", type=str, metavar='<str>', default='mean', help=\"The aggregation method for regp and bregp types (mot|attsum|attmean) (default=mot)\")\n",
    "        self.parser.add_argument(\"--dropout\", dest=\"dropout_prob\", type=float, metavar='<float>', default=0.5, help=\"The dropout probability. To disable, give a negative number (default=0.5)\")\n",
    "        self.parser.add_argument(\"--pretrained\", dest=\"pretrained\", type=int, metavar='<int>', default=1, help=\"Whether to use pretrained or not\")\n",
    "        self.parser.add_argument(\"--epochs\", dest=\"epochs\", type=int, metavar='<int>', default=50, help=\"Number of epochs (default=50)\")\n",
    "        self.parser.add_argument(\"--attention_width\", dest=\"attention_width\", type=int, metavar='<int>', default=5, help=\"Width of attention (default=5)\")\n",
    "        self.parser.add_argument(\"--maxlen\", dest=\"maxlen\", type=int, metavar='<int>', default=0, help=\"Maximum allowed number of words during training. '0' means no limit (default=0)\")\n",
    "        self.parser.add_argument('--gpu', dest='gpu', type=int, metavar='<int>', default=0, help=\"Specify which GPU to use (default=0)\")\n",
    "        self.parser.add_argument(\"--hdim\", dest='hidden_layer_size', type=int, metavar='<int>', default=300, help=\"Hidden layer size (default=50)\")\n",
    "        self.parser.add_argument(\"--lr\", dest='learn_rate', type=float, metavar='<float>', default=0.001, help=\"Learning Rate\")\n",
    "        self.parser.add_argument(\"--clip_norm\", dest='clip_norm', type=int, metavar='<int>', default=0, help=\"Clip Norm value\")\n",
    "        self.parser.add_argument(\"--trainable\", dest='trainable', type=int, metavar='<int>', default=1, help=\"Trainable Word Embeddings (0|1)\")\n",
    "        self.parser.add_argument('--l2_reg', dest='l2_reg', type=float, metavar='<float>', default=0.0, help='L2 regularization, default=0')\n",
    "        self.parser.add_argument('--eval', dest='eval', type=int, metavar='<int>', default=1, help='Epoch to evaluate results')\n",
    "        self.parser.add_argument('--log', dest='log', type=int, metavar='<int>', default=1, help='1 to output to file and 0 otherwise')\n",
    "        self.parser.add_argument('--dev', dest='dev', type=int, metavar='<int>', default=1, help='1 for development set 0 to train-all')\n",
    "        self.parser.add_argument('--cuda', action='store_true', help='use CUDA')\n",
    "        self.parser.add_argument('--seed', type=int, default=1111, help='random seed')\n",
    "        self.parser.add_argument('--toy', action='store_true', help='Use toy dataset (for fast testing)')\n",
    "#         self.args = self.parser.parse_args()\n",
    "        self.args = self.parser.parse_args(['--batch-size', '2', '--rnn_type', 'GRU', '--cuda', '--gpu', '1', '--lr', '0.0001', '--mdl', 'RNN', '--clip_norm', '1', '--opt', 'Adam'])\n",
    "        # Set the random seed manually for reproducibility.\n",
    "        torch.manual_seed(self.args.seed)\n",
    "        if torch.cuda.is_available():\n",
    "            if not self.args.cuda:\n",
    "                print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "            else:\n",
    "                print(\"There are {} CUDA devices\".format(torch.cuda.device_count()))\n",
    "                if(self.args.gpu > 0):\n",
    "                    print(\"Setting torch GPU to {}\".format(self.args.gpu))\n",
    "                    torch.cuda.set_device(self.args.gpu)\n",
    "                    print(\"Using device:{} \".format(torch.cuda.current_device()))\n",
    "                torch.cuda.manual_seed(self.args.seed)\n",
    "\n",
    "        np.random.seed(self.args.seed)\n",
    "        random.seed(self.args.seed)\n",
    "        # Load Data files for training\n",
    "        if(self.args.toy):\n",
    "            file_path = './store/{}_{}_{}.pkl'.format(self.args.dataset, self.args.mode, 'toy')\n",
    "        else:\n",
    "            file_path = './store/{}_{}.pkl'.format(self.args.dataset, self.args.mode)\n",
    "\n",
    "        with open(file_path,'rb') as f:\n",
    "            self.env = pickle.load(f)\n",
    "\n",
    "        print('Stored Environment:{}'.format(self.env.keys()))\n",
    "        self.train_set = self.env['train']\n",
    "        self.test_set = self.env['test']\n",
    "        self.dev_set = self.env['dev']\n",
    "\n",
    "        if(self.args.dev==0):\n",
    "            self.train_set = self.train_set + self.dev_set\n",
    "\n",
    "        print(\"Loaded environment\")\n",
    "        print(\"Creating Model...\")\n",
    "        self.model_name = self.args.aggregation + '_' + self.args.model_type\n",
    "        if(self.args.model_type=='TD-RNN'):\n",
    "            self.mdl = TargetRNN(self.args, len(self.env['word_index']),pretrained=self.env['glove'])\n",
    "        elif(self.args.model_type in ['RNN','ATT-RNN']):\n",
    "            self.mdl = BasicRNN(self.args, len(self.env['word_index']),pretrained=self.env['glove'])\n",
    "        if(self.args.cuda):\n",
    "            self.mdl.cuda()\n",
    "\n",
    "    def make_dir(self):\n",
    "        if(self.args.log==1):\n",
    "            self.out_dir = './logs/{}/{}/{}/{}/'.format(self.mode, self.dataset, self.model_name, self.uuid)\n",
    "            self.mkdir_p(self.out_dir)\n",
    "            self.mdl_path = self.out_dir + '/mdl.ckpt'  # What is the new file format?\n",
    "            self.path = self.out_dir + '/logs.txt'\n",
    "            self.print_args(self.args, path=self.path)\n",
    "\n",
    "    def write_to_file(self, txt):\n",
    "        if(self.args.log==1):\n",
    "            with open(self.path,'a+') as f:\n",
    "                f.write(txt + '\\n')\n",
    "        print(txt)\n",
    "\n",
    "    def print_args(self, args, path=None):\n",
    "        if path:\n",
    "            output_file = open(path, 'w')\n",
    "        args.command = ' '.join(sys.argv)\n",
    "        items = vars(args)\n",
    "        output_file.write('=============================================== \\n')\n",
    "        for key in sorted(items.keys(), key=lambda s: s.lower()):\n",
    "            value = items[key]\n",
    "            if not value:\n",
    "                value = \"None\"\n",
    "            if path is not None:\n",
    "                output_file.write(\"  \" + key + \": \" + str(items[key]) + \"\\n\")\n",
    "        output_file.write('=============================================== \\n')\n",
    "        if path:\n",
    "            output_file.close()\n",
    "        del args.command\n",
    "    \n",
    "    def mkdir_p(self, path):\n",
    "        if path == '':\n",
    "            return\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError as exc: # Python >2.5\n",
    "            if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "                pass\n",
    "            else: raise\n",
    "\n",
    "    def evaluate(self, x, eval_type='test'):\n",
    "        ''' Evaluates normal RNN model\n",
    "        '''\n",
    "        hidden = self.mdl.init_hidden(len(x))\n",
    "        sentence, targets, actual_batch = self.make_batch(x, -1, evaluation=True)\n",
    "        output, hidden = self.mdl(sentence, hidden)\n",
    "        loss = self.criterion(output, targets).data\n",
    "        print(\"Test loss={}\".format(loss[0]))\n",
    "        accuracy = self.get_accuracy(output, targets)\n",
    "\n",
    "    def evaluate_target(self, x, eval_type='test'):\n",
    "        ''' Evaluates Target-RNN model\n",
    "        '''\n",
    "        sentence, targets, actual_batch = self.make_target_batch(x, -1, evaluation=True)\n",
    "        left_input, right_input = sentence[0], sentence[1]\n",
    "        if(sentence is None):\n",
    "            return None\n",
    "        left_hidden = self.mdl.init_hidden(actual_batch)\n",
    "        right_hidden = self.mdl.init_hidden(actual_batch)\n",
    "        left_hidden = repackage_hidden(left_hidden)\n",
    "        right_hidden = repackage_hidden(right_hidden)\n",
    "        output = self.mdl(left_input, right_input, left_hidden, right_hidden)\n",
    "        loss = self.criterion(output, targets).data[0]\n",
    "        print(\"Test loss={}\".format(loss))\n",
    "        accuracy = self.get_accuracy(output, targets)\n",
    "       \n",
    "    def get_accuracy(self, output, targets):\n",
    "        output = tensor_to_numpy(output)\n",
    "        targets = tensor_to_numpy(targets)\n",
    "        output = np.argmax(output, axis=1)\n",
    "        dist = dict(Counter(output))\n",
    "        print(\"Output Distribution={}\".format(dist))\n",
    "        acc = accuracy_score(targets, output)\n",
    "        print(\"Accuracy={}\".format(acc))\n",
    "        return acc\n",
    "\n",
    "    def pad_to_batch_max(self, x):\n",
    "        lengths = [len(y) for y in x]\n",
    "        max_len = np.max(lengths)\n",
    "        padded_tokens = sequence.pad_sequences(x, maxlen=max_len)\n",
    "        return torch.LongTensor(padded_tokens.tolist()).transpose(0,1)\n",
    "\n",
    "    def make_target_batch(self, x, i, evaluation=False):\n",
    "        ''' target dependent batches\n",
    "        '''\n",
    "        if(i>=0):\n",
    "            batch = x[int(i * self.args.batch_size):int(i * self.args.batch_size)+self.args.batch_size]\n",
    "        else:\n",
    "            batch = x\n",
    "        if(len(batch)==0):\n",
    "            return None, None, self.args.batch_size\n",
    "        left_tensor = self.pad_to_batch_max([x['left'] for x in batch])\n",
    "        right_tensor = self.pad_to_batch_max([x['right'] for x in batch][::-1])\n",
    "        targets = torch.LongTensor(np.array([x['polarity'] for x in batch], dtype=np.int32).tolist())\n",
    "        assert(left_tensor.size(1)==right_tensor.size(1))\n",
    "        actual_batch = left_tensor.size(1)\n",
    "        if(self.args.cuda):\n",
    "            left_tensor = left_tensor.cuda()\n",
    "            right_tensor = right_tensor.cuda()\n",
    "            targets = targets.cuda()  \n",
    "        left_tensor = Variable(left_tensor)\n",
    "        right_tensor = Variable(right_tensor)\n",
    "        targets = Variable(targets, volatile=evaluation)      \n",
    "        return [left_tensor, right_tensor], targets, actual_batch\n",
    "\n",
    "    def make_batch(self, x, i, evaluation=False):\n",
    "        ''' -1 to take all\n",
    "        '''\n",
    "        if(i>=0):\n",
    "            batch = x[int(i * self.args.batch_size):int(i * self.args.batch_size)+self.args.batch_size]\n",
    "        else:\n",
    "            \n",
    "            batch = x\n",
    "        if(len(batch)==0):\n",
    "            return None,None, self.args.batch_size\n",
    "\n",
    "        sentence = self.pad_to_batch_max([x['tokenized_txt'] for x in batch])\n",
    "        targets = torch.LongTensor(np.array([x['polarity'] for x in batch], dtype=np.int32).tolist())\n",
    "        actual_batch = sentence.size(1)\n",
    "        if(self.args.cuda):\n",
    "            sentence = sentence.cuda()\n",
    "            targets = targets.cuda()  \n",
    "        sentence = Variable(sentence)\n",
    "        targets = Variable(targets, volatile=evaluation)      \n",
    "        return sentence, targets, actual_batch\n",
    "\n",
    "    def select_optimizer(self):\n",
    "        if(self.args.opt=='Adam'):\n",
    "            self.optimizer =  optim.Adam(self.mdl.parameters(), lr=self.args.learn_rate)\n",
    "        elif(self.args.opt=='RMS'):\n",
    "            self.optimizer =  optim.RMSprop(self.mdl.parameters(), lr=self.args.learn_rate)\n",
    "        elif(self.args.opt=='SGD'):\n",
    "            self.optimizer =  optim.SGD(self.mdl.parameters(), lr=self.args.learn_rate)\n",
    "        elif(self.args.opt=='Adagrad'):\n",
    "            self.optimizer =  optim.Adagrad(self.mdl.parameters(), lr=self.args.learn_rate)\n",
    "        elif(self.args.opt=='Adadelta'):\n",
    "            self.optimizer =  optim.Adadelta(self.mdl.parameters(), lr=self.args.learn_rate)\n",
    "\n",
    "    def train_target_batch(self, i):\n",
    "        ''' Trains a regular Target-Dependent RNN model\n",
    "        '''\n",
    "        sentence, targets, actual_batch = self.make_target_batch(self.train_set, i)\n",
    "        left_input, right_input = sentence[0], sentence[1]\n",
    "        if(sentence is None):\n",
    "            return None\n",
    "\n",
    "        # Do I need to init both? Can I just pass in 0 vectors?\n",
    "        left_hidden = self.mdl.init_hidden(actual_batch)\n",
    "        right_hidden = self.mdl.init_hidden(actual_batch)\n",
    "\n",
    "        left_hidden = repackage_hidden(left_hidden)\n",
    "        right_hidden = repackage_hidden(right_hidden)\n",
    "        self.mdl.zero_grad()\n",
    "        output = self.mdl(left_input, right_input, left_hidden, right_hidden)\n",
    "        loss = self.criterion(output, targets)\n",
    "        loss.backward()\n",
    "        if(self.args.clip_norm>0):\n",
    "            coeff = clip_gradient(self.mdl, self.args.clip_norm)\n",
    "            for p in self.mdl.parameters():\n",
    "                p.grad.mul_(coeff)\n",
    "        self.optimizer.step()\n",
    "        return loss.data[0]\n",
    "\n",
    "    def train_batch(self, i):\n",
    "        ''' Trains a regular RNN model\n",
    "        '''\n",
    "        sentence, targets, actual_batch = self.make_batch(self.train_set, i)\n",
    "        if(sentence is None):\n",
    "            return None\n",
    "        hidden = self.mdl.init_hidden(actual_batch)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        self.mdl.zero_grad()\n",
    "        output, hidden = self.mdl(sentence, hidden)\n",
    "        loss = self.criterion(output, targets)\n",
    "        loss.backward()\n",
    "        if(self.args.clip_norm>0):\n",
    "            coeff = clip_gradient(self.mdl, self.args.clip_norm)\n",
    "            for p in self.mdl.parameters():\n",
    "                if p.grad is not None:\n",
    "                    p.grad.mul_(coeff)\n",
    "        self.optimizer.step()\n",
    "        return loss.data[0]\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Starting training\")\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        print(self.args)\n",
    "        total_loss = 0\n",
    "        num_batches = int(len(self.train_set) / self.args.batch_size) + 1\n",
    "        self.select_optimizer()\n",
    "        for epoch in range(1,self.args.epochs+1):\n",
    "            t0 = time.clock()\n",
    "            random.shuffle(self.train_set)\n",
    "            print(\"========================================================================\")\n",
    "            losses = []\n",
    "            actual_batch = self.args.batch_size\n",
    "            for i in range(num_batches):\n",
    "                if(self.args.model_type in ['TD-RNN']):\n",
    "                    loss = self.train_target_batch(i)\n",
    "                else:\n",
    "                    loss = self.train_batch(i)\n",
    "                if(loss is None):\n",
    "                    continue    \n",
    "                losses.append(loss)\n",
    "            t1 = time.clock()\n",
    "            print(\"[Epoch {}] Train Loss={} T={}s\".format(epoch, np.mean(losses),t1-t0))\n",
    "            if(epoch >0 and epoch % self.args.eval==0):\n",
    "                if(self.args.model_type in ['TD-RNN']):\n",
    "                    self.evaluate_target(self.test_set)\n",
    "                else:\n",
    "                    self.evaluate(self.test_set)\n",
    "               \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    exp = BaseExperiment()\n",
    "    exp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"--batch-size 20 --rnn_type GRU --cuda --gpu 1 --lr 0.0001 --mdl RNN --clip_norm 1 --opt Adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--batch-size', '20', '--rnn_type', 'GRU', '--cuda', '--gpu', '1', '--lr', '0.0001', '--mdl', 'RNN', '--clip_norm', '1', '--opt', 'Adam']\n"
     ]
    }
   ],
   "source": [
    "print(a.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9507, 100)\n",
      "9506 9507\n",
      "load word-to-id done!\n",
      "Counter({'0': 3127, '1': 1561, '-1': 1560})\n",
      "load word-to-id done!\n",
      "Counter({'0': 346, '-1': 173, '1': 173})\n"
     ]
    }
   ],
   "source": [
    "from utils2 import load_w2v, batch_index, load_inputs_twitter, load_word_id_mapping\n",
    "\n",
    "max_sentence_len=50\n",
    "embedding_dim=100\n",
    "word_id_mapping, w2v = load_w2v('twitter/twitter_word_embedding_partial_100.txt', embedding_dim)\n",
    "\n",
    "tr_x, tr_sen_len, tr_y = load_inputs_twitter(\n",
    "    'twitter/train.raw',\n",
    "    word_id_mapping,\n",
    "    max_sentence_len\n",
    ")\n",
    "te_x, te_sen_len, te_y = load_inputs_twitter(\n",
    "    'twitter/test.raw',\n",
    "    word_id_mapping,\n",
    "    max_sentence_len\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./store/Restaurants_term.pkl','rb') as f:\n",
    "    env = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = env['train']\n",
    "test_set = env['test']\n",
    "dev_set = env['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3602"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(te_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4559, 1120, 1675, 8396, 4361, 5153, 5151, 4016, 1412,  465, 3689,\n",
       "       6632, 8879,   22,  465,  358, 4559, 6798, 9002, 4669, 7315,  799,\n",
       "       4905, 4559, 5205, 2693, 5795, 1001, 8396,  465,  358,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_x[691]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_batch(self,x, i, evaluation=False):\n",
    "evaluation=False\n",
    "from keras.preprocessing import sequence\n",
    "def pad_to_batch_max(x):\n",
    "    lengths = [len(y) for y in x]\n",
    "    max_len = np.max(lengths)\n",
    "    padded_tokens = sequence.pad_sequences(x, maxlen=max_len)\n",
    "    return torch.LongTensor(padded_tokens.tolist()).transpose(0,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Vanzytay\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "x = train_set\n",
    "i = 2\n",
    "batch_size = 10\n",
    "if(i>=0):\n",
    "    batch = x[int(i * batch_size):int(i * batch_size)+batch_size]\n",
    "else:\n",
    "    batch = x\n",
    "# if(len(batch)==0):\n",
    "#     return None,None, batch_size\n",
    "import numpy as np\n",
    "sentence = pad_to_batch_max([x['tokenized_txt'] for x in batch])\n",
    "targets = torch.LongTensor(np.array([x['polarity'] for x in batch], dtype=np.int32).tolist())\n",
    "actual_batch = sentence.size(1)\n",
    "if(True):\n",
    "    sentence = sentence.cuda()\n",
    "    targets = targets.cuda()  \n",
    "sentence = Variable(sentence)\n",
    "targets = Variable(targets, volatile=evaluation)      \n",
    "# return sentence, targets, actual_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  897   897     0     0     0     0     0     0     0     0\n",
       "  522   522     0     0     0     0     0     0     0     0\n",
       " 1661  1661     0     0     0     0     0     0     0     0\n",
       " 2493  2493     0     0     0     0     0     0     0     0\n",
       " 2783  2783     0     0     0     0     0     0     0     0\n",
       " 2597  2597     0     0     0     0     0  2568  2568     0\n",
       " 1981  1981     0     0     0     0     0   964   964   964\n",
       " 4276  4276     0     0     0     0     0  2245  2245  4827\n",
       " 1217  1217     0     0     0     0     0   391   391  3313\n",
       " 4447  4447     0     0  5009   964   964  1645  1645  3956\n",
       " 1159  1159     0     0  4360   275   275   964   964   907\n",
       " 4209  4209     0     0  4413   322   322  1886  1886  2523\n",
       " 3002  3002   964   964  2378   964   964  3474  3474  3313\n",
       "  762   762  2660  2660  1843   738   738  2781  2781  2614\n",
       " 4328  4328  1843  1843   322   907   907  2388  2388   789\n",
       " 4132  4132  4493  4493  1727  2523  2523   897   897   964\n",
       " 4425  4425   322   322  3324  3601  3601   391   391  4657\n",
       " 1381  1381   971   971  4064  4098  4098  2597  2597  4064\n",
       " 1843  1843  2495  2495   964  4953  4953   964   964    60\n",
       " 1603  1603   384   384  3484   275   275  2370  2370  1101\n",
       "    2     2     2     2     2     2     2     2     2     2\n",
       "[torch.cuda.LongTensor of size 21x10 (GPU 0)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'polarity': 2, 'right': [2940, 322, 1243, 4539, 2464, 2864, 2615, 2113, 779, 4447, 4727, 1050, 415, 3769, 4863, 2523, 3069, 3601, 3657, 5017, 897, 2081, 4064, 964, 655, 2403, 1662, 2], 'actual_len': 29, 'term_id': [2940], 'tokenized_txt': [964, 2940, 322, 1243, 4539, 2464, 2864, 2615, 2113, 779, 4447, 4727, 1050, 415, 3769, 4863, 2523, 3069, 3601, 3657, 5017, 897, 2081, 4064, 964, 655, 2403, 1662, 2], 'term_len': 1, 'pointers': [1, 1], 'left': [964, 2940]}\n"
     ]
    }
   ],
   "source": [
    "print(train_set[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
